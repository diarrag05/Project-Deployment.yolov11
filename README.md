# Projet de DÃ©tection de Chips et Trous (YOLOv11)

Application Flask pour l'analyse d'images de composants Ã©lectroniques avec dÃ©tection automatique de chips et de trous, calcul du taux de vide (void rate) et segmentation assistÃ©e par SAM (Segment Anything Model).

## ğŸ“‹ Table des matiÃ¨res

- [PrÃ©requis](#prÃ©requis)
- [Installation](#installation)
- [Configuration](#configuration)
- [DÃ©marrage de l'application](#dÃ©marrage-de-lapplication)
- [ScÃ©narios d'utilisation](#scÃ©narios-dutilisation)
- [YOLOv11 Fine-tuning: Complete Guide](#-yolov11-fine-tuning-complete-guide)
- [Scripts d'entraÃ®nement et d'Ã©valuation](#-scripts-dentraÃ®nement-et-dÃ©valuation)
- [Explication des calculs](#explication-des-calculs)
- [Structure du projet](#structure-du-projet)
- [Notes importantes](#notes-importantes)
- [Endpoints API](#endpoints-api-principaux)

## ğŸ”§ PrÃ©requis

- Python 3.8 ou supÃ©rieur
- pip (gestionnaire de paquets Python)
- GPU NVIDIA avec CUDA (recommandÃ© pour l'entraÃ®nement et l'infÃ©rence rapide)
  - Alternative : CPU (plus lent mais fonctionnel)
  - Alternative : Apple Silicon avec MPS (supportÃ©)

## ğŸ“¦ Installation

### 1. Cloner le projet

```bash
git clone <url-du-repo>
cd Project-Deployment.yolov11
```

### 2. CrÃ©er un environnement virtuel (recommandÃ©)

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python -m venv venv
source venv/bin/activate
```

### 3. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

**Note importante** : L'installation peut prendre plusieurs minutes car elle inclut :
- PyTorch et ses dÃ©pendances
- Ultralytics (YOLOv11)
- Segment Anything Model (SAM) depuis GitHub

### 4. VÃ©rifier la configuration

Le fichier `.env` est dÃ©jÃ  prÃ©sent dans le projet avec les paramÃ¨tres par dÃ©faut. Vous pouvez le modifier si nÃ©cessaire (voir section [Configuration](#configuration)).

## âš™ï¸ Configuration

Le projet utilise un fichier `.env` pour la configuration. Les valeurs par dÃ©faut sont dÃ©jÃ  dÃ©finies, mais vous pouvez les personnaliser :

### Variables principales

- `FLASK_HOST` : Adresse IP du serveur (dÃ©faut: `127.0.0.1`)
- `FLASK_PORT` : Port du serveur (dÃ©faut: `5000`)
- `FLASK_DEBUG` : Mode debug (dÃ©faut: `True`)
- `TRAINING_EPOCHS` : Nombre d'Ã©poques pour l'entraÃ®nement (dÃ©faut: `100`)
- `TRAINING_BATCH_SIZE` : Taille du batch (dÃ©faut: `8`)
- `TRAINING_PATIENCE` : Patience pour l'early stopping (dÃ©faut: `30`)
- `VOID_RATE_THRESHOLD` : Seuil de void rate en pourcentage (dÃ©faut: `5.0`)

Voir `backend/src/config.py` pour la liste complÃ¨te des variables configurables.

## ğŸš€ DÃ©marrage de l'application

### Option 1 : Via le script de dÃ©marrage (recommandÃ©)

```bash
python api/run_api.py
```

### Option 2 : Via le module Flask directement

```bash
python api/app.py
```

### Option 3 : Via Flask CLI

```bash
cd api
flask run
```

L'application sera accessible sur `http://127.0.0.1:5000` (ou l'adresse configurÃ©e dans `.env`).

### Interface web

Ouvrez votre navigateur et accÃ©dez Ã  :
- **Interface principale** : `http://127.0.0.1:5000/`
- **Health check** : `http://127.0.0.1:5000/health`
- **API endpoints** : `http://127.0.0.1:5000/api/...`

## ğŸ“Š ScÃ©narios d'utilisation

### ScÃ©nario 1 : Premier dÃ©marrage (aucun modÃ¨le)

**Situation** : Vous clonez le projet pour la premiÃ¨re fois, le dossier `models/` ne contient pas de modÃ¨le finetunÃ© (`best.pt`).

**Comportement automatique** :
1. L'utilisateur tente une analyse d'image via l'API
2. L'API dÃ©tecte l'absence de `models/best.pt`
3. **L'entraÃ®nement initial est lancÃ© automatiquement en arriÃ¨re-plan**
4. L'API retourne une erreur **503** avec le message :
   ```json
   {
     "error": "No model found. Initial training has been started automatically.",
     "training_id": "<uuid>",
     "message": "Please wait for training to complete, then try again."
   }
   ```
5. L'utilisateur doit attendre la fin de l'entraÃ®nement avant de pouvoir analyser des images

**DurÃ©e estimÃ©e** : L'entraÃ®nement initial peut prendre de 30 minutes Ã  plusieurs heures selon :
- La puissance du GPU/CPU
- Le nombre d'Ã©poques configurÃ©
- La taille du dataset

**Suivi de l'entraÃ®nement** :
- Consultez les logs dans `logs/training.log`
- Utilisez l'endpoint `/api/training/status/<training_id>` pour vÃ©rifier le statut

### ScÃ©nario 2 : ModÃ¨le existant

**Situation** : Le fichier `models/best.pt` existe dÃ©jÃ .

**Comportement** :
- L'analyse fonctionne normalement avec le modÃ¨le finetunÃ©
- Aucun entraÃ®nement n'est dÃ©clenchÃ©
- Les performances sont optimales

### ScÃ©nario 3 : RÃ©entraÃ®nement

**Situation** : L'utilisateur souhaite rÃ©entraÃ®ner le modÃ¨le avec de nouvelles donnÃ©es.

**Comportement automatique** :
- Si `models/best.pt` existe : **fine-tuning** (entraÃ®nement continu depuis le modÃ¨le existant)
- Si `models/best.pt` n'existe pas : **entraÃ®nement depuis zÃ©ro** avec `models/yolo11s-seg.pt` (tÃ©lÃ©chargÃ© automatiquement)

**Important** : 
- Le modÃ¨le rÃ©entraÃ®nÃ© **Ã©crase** le prÃ©cÃ©dent `best.pt`
- Il n'y a pas besoin de checkbox pour forcer le modÃ¨le prÃ©-entraÃ®nÃ©, l'application gÃ¨re automatiquement

**Lancement du rÃ©entraÃ®nement** :
- Via l'API : `POST /api/training/retrain`
- Via le script : `python backend/train.py --epochs 100 --batch 8 --patience 30`

## ğŸ“ YOLOv11 Fine-tuning: Complete Guide

### Introduction

This project implements an **automatic defect detection and segmentation system** for electronic components using the **YOLOv11-segmentation** model. The main objective is to identify components (chips) and defects (holes/voids) present in these components, then automatically calculate the **void rate** for quality assessment.

### Why this project?

In the electronics industry, **voids in solder joints** can cause component failures. Automatic detection of these defects is crucial for:
- ğŸ” Automated quality control
- ğŸ“Š Void rate calculation
- âš¡ Reduction of manual inspection costs
- ğŸ¯ Improvement of product reliability

### Dataset and Data Preparation

#### Data Source
- **Origin**: Custom dataset of electronic components with defects
- **Annotation Tool**: Roboflow platform
- **Annotation Process**: Manual polygon-based annotation for each component (chip) and defect (hole/void)
- **Export Format**: YOLOv11 segmentation format (normalized polygon coordinates)

#### Annotation Format

**YOLO Segmentation Format** (normalized polygons):
```
class_id x1 y1 x2 y2 x3 y3 ... xn yn
```

Example of a hole annotation:
```
1 0.4527 0.3892 0.4634 0.3901 0.4729 0.3987 0.4527 0.3892
```
- `1` = class "hole-JsHt"
- (x, y) coordinates normalized between 0 and 1

#### Dataset Distribution

| Split | Number of images | Percentage |
|-------|-----------------|------------|
| **Train** | 66 | 68% |
| **Validation** | 20 | 21% |
| **Test** | 11 | 11% |
| **Total** | **97** | **100%** |

#### Classes

| ID | Class name | Description |
|----|------------|-------------|
| 0 | `chip` | Electronic components |
| 1 | `hole-JsHt` | Holes/voids in components |

### Architecture and Technologies

#### Technologies Used

| Technology | Version | Usage |
|-----------|---------|-------|
| **Python** | 3.8+ | Main language |
| **Ultralytics** | â‰¥8.0.0 | YOLOv11 framework |
| **PyTorch** | â‰¥2.0.0 | Deep learning backend |
| **OpenCV** | â‰¥4.8.0 | Image processing |
| **Matplotlib** | â‰¥3.7.0 | Visualization |

#### YOLOv11-Segmentation Model

**Why YOLOv11-seg?**
- ğŸš€ **Fast**: Real-time inference
- ğŸ¯ **Accurate**: State-of-the-art for instance segmentation
- ğŸ“¦ **Compact**: Small model (11s) with 11.6M parameters
- ğŸ”„ **Pre-trained**: On COCO dataset (80 classes)

**Architecture**:
- **Backbone**: CSPDarknet with P2-P5 feature pyramids
- **Neck**: PAN (Path Aggregation Network)
- **Head**: Dual heads for detection + segmentation
- **Output**: Bounding boxes + segmentation masks

### Work Completed

#### Step 1: Exploration and Understanding
**Objective**: Understand YOLOv11 fine-tuning and analyze data

**Actions performed**:
- ğŸ“¹ Study of video tutorial on YOLOv11 fine-tuning
- ğŸ“‚ Analysis of dataset structure (97 images, segmentation format)
- ğŸ” Verification of annotations (polygons vs bounding boxes)
- ğŸ“Š Statistics: 2 classes, train/val/test distribution

**Results**:
```
Dataset Statistics:
- Total images: 97
- Train: 66 images (68%)
- Validation: 20 images (21%)
- Test: 11 images (11%)
- Classes: chip, hole-JsHt
- Format: YOLOv11 segmentation (polygons)
```

#### Step 2: Model Selection
**Problem**: Choose between YOLOv11n/s/m/l/x

**Decision**:
- Initially: **YOLOv11m** (medium, more accurate)
- Finally: **YOLOv11s** (small, faster)

**Reason for change**:
```
YOLOv11m: ~3.3 hours training time (CPU)
YOLOv11s: ~1.5 hours training time (CPU)
```
â†’ Time savings with acceptable performance for 97 images

#### Step 3: Training Configuration

**File created**: `train.py`

**Optimal configuration found**:
```python
model = YOLO("models/yolo11s-seg.pt")  # âš ï¸ Important: -seg for segmentation

config = {
    'data': 'dataset/data.yaml',
    'epochs': 100,
    'batch': 8,              # Reduced from 16 â†’ 8 for stability
    'imgsz': 640,
    'device': 'cpu',         # CPU instead of MPS (see issues)
    'optimizer': 'AdamW',
    'lr0': 0.001,
    'patience': 30,          # Early stopping
    'amp': False,            # Disabled for MPS compatibility
}
```

**Configuration file**: `data.yaml`
```yaml
train: ../dataset/train/images
val: ../dataset/valid/images
test: ../dataset/test/images
nc: 2
names:
  0: chip
  1: hole-JsHt
```

#### Step 4: Model Training

**Execution command**:
```bash
python train.py
# or
python backend/train.py
```

**Training duration**: ~1.067 hours (64 minutes)

**Early stopping**:
- Configured: 30 epochs patience
- Stopped at: Epoch 62 (out of 100 max)
- Best model: Epoch 32

**Metrics monitored during training**:
- Box Loss (bounding box localization)
- Seg Loss (segmentation mask quality)
- Class Loss (chip vs hole classification)
- mAP50 and mAP50-95

**Generated files**:
```
runs/segment/train/
â”œâ”€â”€ weights/
â”‚   â”œâ”€â”€ best.pt          # Best model (epoch 32)
â”‚   â””â”€â”€ last.pt          # Last model (epoch 62)
â”œâ”€â”€ results.csv          # Metrics per epoch
â”œâ”€â”€ confusion_matrix.png # Confusion matrix
â”œâ”€â”€ results.png          # Training curves
â”œâ”€â”€ PR_curve.png         # Precision-Recall curves
â”œâ”€â”€ F1_curve.png         # F1-Score curves
â””â”€â”€ val_batch*.jpg       # Prediction examples
```

#### Step 5: Model Evaluation

**File created**: `evaluate.py`

**Evaluation command**:
```bash
python evaluate.py --model runs/segment/train/weights/best.pt
```

**Metrics calculated**:
- mAP50 and mAP50-95 (box & mask)
- Precision and Recall per class
- F1-Score per class
- Confusion matrix

#### Step 6: Inference Script

**File created**: `inference.py`

**Usage**:
```bash
# Prediction on one image
python inference.py --source dataset/test/images/image.jpg

# With custom confidence threshold
python inference.py --source dataset/test/images/image.jpg --conf 0.5 --iou 0.7
```

### Results and Analysis

#### Overall Performance

**Successful training** with the following metrics:

| Metric Type | mAP50 | mAP50-95 | Precision | Recall |
|-------------|-------|----------|-----------|--------|
| **Bounding Box** | 88.0% | 72.7% | 96.8% | 72.0% |
| **Segmentation Mask** | 87.3% | 64.1% | 96.2% | 72.0% |

#### Per-Class Analysis

##### Class "chip" (Components)
```
âœ… Precision: 95.6%  â†’ Model makes few false detections
âœ… Recall: 100%      â†’ Model detects all components
âœ… F1-Score: 97.73%  â†’ Excellent balance
```

**Interpretation**:
The model is **excellent** at detecting electronic components. It doesn't miss any component (100% recall) and makes very few errors (95.6% precision).

##### Class "hole-JsHt" (Holes/Voids)
```
âœ… Precision: 97.8%  â†’ Model makes very few false detections
âš ï¸  Recall: 43.9%    â†’ Model misses 56% of holes
âš ï¸  F1-Score: 60.60% â†’ Moderate performance
```

**Interpretation**:
The model is **very conservative** in detecting holes:
- When it detects a hole, it's correct 97.8% of the time (excellent precision)
- **BUT** it misses more than half of the holes present (low recall)

**Why this imbalance?**
1. ğŸ”¢ **Class imbalance**: Likely more chips than holes in dataset
2. ğŸ“ **Object size**: Holes are smaller and harder to detect
3. ğŸ“Š **Limited data**: Only 97 images total

#### Results Visualization

**Precision-Recall Curve (PR Curve)**:
```
runs/segment/train/PR_curve.png
```
- Area under curve = mAP
- Closer the curve to top-right corner, the better

**F1-Score Curve**:
```
runs/segment/train/F1_curve.png
```
- Shows best precision-recall trade-off
- Peak of curve = optimal confidence threshold

**Confusion Matrix**:
```
runs/segment/train/confusion_matrix.png
```

Example confusion matrix:
```
                Predicted
              chip   hole   background
Actual chip     50     0        0      â† Perfect!
Actual hole      0    15       19      â† 19 holes missed
       BG        1     2       --
```

#### Training Curves

**Loss Evolution**:
```
runs/segment/train/results.png
```

Expected observation:
- âœ… Box Loss â†“ : Localization improvement
- âœ… Seg Loss â†“ : Mask improvement
- âœ… Class Loss â†“ : Classification improvement
- âœ… Stable convergence without overfitting

### Improvement Recommendations

**To improve hole recall**:

1. **Collect more data**:
   ```
   Current dataset: 97 images
   Recommended: 300-500 images
   ```

2. **Increase data augmentation** (in `train.py`):
   ```python
   mosaic=1.0,        # Mix 4 images
   mixup=0.1,         # Add mixup
   copy_paste=0.1,    # Copy-paste objects
   ```

3. **Adjust confidence threshold** (inference):
   ```bash
   # More permissive for holes
   python inference.py --source dataset/test/images/image.jpg --conf 0.15  # instead of 0.25
   ```

4. **Use larger model**:
   ```python
   model = YOLO("models/yolo11m-seg.pt")  # Medium instead of Small
   ```

### Issues Encountered and Solutions

#### Issue 1: Shape Mismatch Error

**Error encountered**:
```
RuntimeError: shape mismatch: value tensor of shape [156542]
cannot be broadcast to indexing result of shape [142122]
```

**Cause**:
- Used **detection** model (`models/yolo11s.pt`)
- While annotations were in **segmentation** format (polygons)

**Solution applied**:
```python
# âŒ Incorrect
model = YOLO("models/yolo11s.pt")  # Detection model

# âœ… Correct
model = YOLO("models/yolo11s-seg.pt")  # Segmentation model
```

**Lesson learned**:
- `.pt` = detection (bounding boxes)
- `-seg.pt` = segmentation (polygon masks)

#### Issue 2: MPS Error on Apple Silicon

**Error encountered**:
```
RuntimeError: view size is not compatible with input tensor's
size and stride (at least one dimension spans across two
contiguous subspaces). Use .reshape(...) instead.
```

**Context**:
- MacBook with M1/M2 chip (Apple Silicon)
- Attempted use of MPS backend (Metal Performance Shaders)

**Solutions attempted**:

1. **First attempt**: Disable AMP
   ```python
   amp=False  # Automatic Mixed Precision
   ```
   **Result**: âŒ Failed, error persists

2. **Second attempt**: Reduce batch size
   ```python
   batch=8  # instead of 16
   ```
   **Result**: âŒ Failed, error persists

3. **Final solution**: Use CPU
   ```python
   device='cpu'  # instead of 'mps'
   ```
   **Result**: âœ… Success

**Technical explanation**:
YOLOv11-segmentation uses complex tensor operations that aren't yet fully supported by PyTorch's MPS backend for segmentation.

**Impact**:
- â±ï¸ Slower training (~1h on CPU vs ~20min on GPU)
- âœ… But works stably

**Future alternative**:
```python
# If you have NVIDIA GPU
device='cuda'  # Much faster
```

#### Issue 3: Relative Paths in data.yaml

**Error encountered**:
```
FileNotFoundError: [Errno 2] No such file or directory:
'../train/images'
```

**Cause**:
Using relative paths in `data.yaml`:
```yaml
# âŒ Problematic
path: .
train: ../train/images
```

**Solution applied**:
```yaml
# âœ… Correct
train: ../dataset/train/images
val: ../dataset/valid/images
test: ../dataset/test/images
```

**Best practice**:
- `train/val/test` = **relative** paths from `data.yaml` file location
- Paths are resolved relative to the `data.yaml` file location

### Development Notes

#### Important Technical Decisions

1. **Choice of YOLOv11s instead of YOLOv11m**
   - Reason: Training time savings (1h vs 3.3h)
   - Trade-off: Slight accuracy decrease acceptable for 97 images

2. **Using CPU instead of MPS**
   - Reason: MPS incompatibility with segmentation operations
   - Impact: Longer training time but stable

3. **Early stopping at 30 epochs**
   - Reason: Avoid overfitting on small dataset
   - Result: Stopped at epoch 62, best model at epoch 32

#### Lessons Learned

1. âœ… **Always verify annotation format** before choosing model
2. âœ… **Use relative paths** in configuration files
3. âœ… **Test on small batch** before launching complete training
4. âœ… **Document encountered issues** for future reference

## ğŸ”¬ Scripts d'entraÃ®nement et d'Ã©valuation

Le projet inclut plusieurs scripts pour l'entraÃ®nement, l'Ã©valuation et l'infÃ©rence :

### EntraÃ®nement (`backend/train.py`)

Script d'entraÃ®nement complet avec gestion automatique des modÃ¨les et dÃ©tection de device.

```bash
# EntraÃ®nement avec paramÃ¨tres par dÃ©faut
python backend/train.py

# EntraÃ®nement avec paramÃ¨tres personnalisÃ©s
python backend/train.py --epochs 150 --batch 16 --patience 50
```

**FonctionnalitÃ©s** :
- DÃ©tection automatique GPU/CPU/MPS
- Utilise `models/best.pt` s'il existe (fine-tuning), sinon `models/yolo11s-seg.pt` (entraÃ®nement depuis zÃ©ro)
- Copie automatique du meilleur modÃ¨le dans `models/best.pt`
- Logging dÃ©taillÃ© dans `logs/training.log`

### Ã‰valuation (`evaluate.py`)

Script pour Ã©valuer les performances du modÃ¨le sur les datasets de validation ou de test.

```bash
# Ã‰valuation sur le set de validation (dÃ©faut)
python evaluate.py

# Ã‰valuation sur le set de test
python evaluate.py --split test

# Ã‰valuation avec un modÃ¨le spÃ©cifique
python evaluate.py --model models/best.pt --split val

# Ã‰valuation avec paramÃ¨tres personnalisÃ©s
python evaluate.py --batch 16 --imgsz 640
```

**MÃ©triques affichÃ©es** :
- mAP50 et mAP50-95 (bounding boxes et masks)
- Precision et Recall globaux
- MÃ©triques par classe (chip, hole-JsHt)
- GÃ©nÃ©ration de graphiques dans `runs/segment/eval_*/`

### InfÃ©rence (`inference.py`)

Script pour faire des prÃ©dictions sur de nouvelles images.

```bash
# PrÃ©diction sur une image
python inference.py --source dataset/test/images/image.jpg

# PrÃ©diction sur un dossier d'images
python inference.py --source dataset/test/images/

# Avec seuils personnalisÃ©s
python inference.py --source dataset/test/images/image.jpg --conf 0.5 --iou 0.7

# Sauvegarder les labels au format YOLO
python inference.py --source dataset/test/images/image.jpg --save-txt
```

**Options disponibles** :
- `--model` : Chemin vers le modÃ¨le (dÃ©faut: `models/best.pt`)
- `--source` : Image, vidÃ©o ou dossier (requis)
- `--conf` : Seuil de confiance (dÃ©faut: 0.25)
- `--iou` : Seuil IoU pour NMS (dÃ©faut: 0.7)
- `--save-txt` : Sauvegarder les labels au format YOLO
- `--imgsz` : Taille d'image (dÃ©faut: 640)

**RÃ©sultats** :
- Images annotÃ©es sauvegardÃ©es dans `runs/segment/predict/`
- Labels (si `--save-txt`) dans `runs/segment/predict/labels/`

### Gestion automatique de SAM

**Comportement** : Si le modÃ¨le SAM (`models/sam_vit_h_4b8939.pth`) n'est pas prÃ©sent :
- Le modÃ¨le est **tÃ©lÃ©chargÃ© automatiquement** lors de la premiÃ¨re utilisation de la segmentation SAM
- Taille du fichier : ~2.4 GB
- TÃ©lÃ©chargement depuis : `https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth`

**Note** : Dans un contexte de production, les modÃ¨les finetunÃ©s (`best.pt`) et SAM seront dÃ©jÃ  prÃ©sents, donc l'utilisateur n'aura pas Ã  faire face Ã  ces lourdes installations. Ce sont des **fallbacks extrÃªmes** pour les cas de dÃ©veloppement ou de dÃ©ploiement initial.

## ğŸ“ Explication des calculs

L'application calcule plusieurs mÃ©triques importantes pour l'analyse des composants :

### 1. **Area (Aire)**

- **DÃ©finition** : Surface totale des chips dÃ©tectÃ©es en pixels
- **Calcul** : Somme des aires de tous les masques de classe "chip" (class_id = 0)
- **UnitÃ©** : PixelsÂ²

### 2. **Void Rate (Taux de vide)**

- **DÃ©finition** : Pourcentage de la surface totale des chips occupÃ©e par les trous
- **Formule** : `Void Rate (%) = (Surface totale des trous / Surface totale des chips) Ã— 100`
- **InterprÃ©tation** :
  - Plus le void rate est Ã©levÃ©, plus la chip est endommagÃ©e
  - Un void rate supÃ©rieur au seuil configurÃ© (`VOID_RATE_THRESHOLD`, dÃ©faut: 5%) indique une chip non utilisable

### 3. **Void % (Pourcentage de vide)**

- **DÃ©finition** : Identique au Void Rate, exprimÃ© en pourcentage
- **Utilisation** : MÃ©trique principale pour dÃ©terminer si une chip est utilisable

### 4. **Max.void % (Pourcentage de vide maximum)**

- **DÃ©finition** : Pourcentage de la surface de la chip occupÃ©e par le **plus grand trou individuel**
- **Formule** : `Max.void % = (Aire du plus grand trou / Aire totale des chips) Ã— 100`
- **InterprÃ©tation** :
  - Indique la taille du dÃ©faut le plus important
  - Utile pour identifier des trous critiques mÃªme si le void rate global est acceptable
  - Peut Ãªtre calculÃ© par chip individuelle ou globalement sur l'image

### Exemple de calcul

```
Image avec :
- 2 chips dÃ©tectÃ©es : Chip A (1000 pxÂ²), Chip B (800 pxÂ²)
- 3 trous dÃ©tectÃ©s : Trou 1 (20 pxÂ²), Trou 2 (30 pxÂ²), Trou 3 (50 pxÂ²)

Calculs globaux :
- Area = 1000 + 800 = 1800 pxÂ²
- Void % = (20 + 30 + 50) / 1800 Ã— 100 = 5.56%
- Max.void % = 50 / 1800 Ã— 100 = 2.78%

Calculs par chip :
Chip A :
- Area = 1000 pxÂ²
- Void % = (20 + 30) / 1000 Ã— 100 = 5.0%
- Max.void % = 30 / 1000 Ã— 100 = 3.0%

Chip B :
- Area = 800 pxÂ²
- Void % = 50 / 800 Ã— 100 = 6.25%
- Max.void % = 50 / 800 Ã— 100 = 6.25%
```

## ğŸ“ Structure du projet

```
Project-Deployment.yolov11/
â”œâ”€â”€ api/                    # Application Flask et routes API
â”‚   â”œâ”€â”€ app.py             # Configuration Flask principale
â”‚   â”œâ”€â”€ routes.py          # Endpoints API
â”‚   â”œâ”€â”€ run_api.py         # Script de dÃ©marrage
â”‚   â”œâ”€â”€ storage.py         # Gestion du stockage des images validÃ©es
â”‚   â””â”€â”€ training_job.py    # Gestion des jobs d'entraÃ®nement
â”‚
â”œâ”€â”€ backend/               # Logique mÃ©tier
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ config.py      # Configuration centralisÃ©e
â”‚   â”‚   â”œâ”€â”€ services/      # Services mÃ©tier
â”‚   â”‚   â”‚   â”œâ”€â”€ yolo_inference.py
â”‚   â”‚   â”‚   â”œâ”€â”€ void_rate_calculator.py
â”‚   â”‚   â”‚   â”œâ”€â”€ sam_segmentation.py
â”‚   â”‚   â”‚   â”œâ”€â”€ training_service.py
â”‚   â”‚   â”‚   â””â”€â”€ label_manager.py
â”‚   â”‚   â”œâ”€â”€ schemas/       # ModÃ¨les de donnÃ©es
â”‚   â”‚   â””â”€â”€ utils/         # Utilitaires
â”‚   â””â”€â”€ train.py           # Script d'entraÃ®nement
â”‚
â”œâ”€â”€ dataset/               # Dataset d'entraÃ®nement (inclus)
â”‚   â”œâ”€â”€ data.yaml          # Configuration YOLO
â”‚   â”œâ”€â”€ train/             # Images et labels d'entraÃ®nement
â”‚   â”œâ”€â”€ valid/             # Images et labels de validation
â”‚   â””â”€â”€ test/              # Images et labels de test
â”‚
â”œâ”€â”€ models/                # ModÃ¨les entraÃ®nÃ©s
â”‚   â”œâ”€â”€ best.pt            # ModÃ¨le YOLO finetunÃ© (gÃ©nÃ©rÃ© aprÃ¨s entraÃ®nement)
â”‚   â””â”€â”€ sam_vit_h_4b8939.pth  # ModÃ¨le SAM (tÃ©lÃ©chargÃ© automatiquement)
â”‚
â”œâ”€â”€ outputs/               # RÃ©sultats et sorties
â”‚   â”œâ”€â”€ uploads/           # Images uploadÃ©es temporairement
â”‚   â”œâ”€â”€ inference/         # Images avec infÃ©rence YOLO
â”‚   â”œâ”€â”€ sam_segmentation/  # RÃ©sultats de segmentation SAM
â”‚   â”œâ”€â”€ results/           # RÃ©sultats d'analyse
â”‚   â””â”€â”€ validated_images/  # Images validÃ©es et leurs labels
â”‚
â”œâ”€â”€ logs/                  # Fichiers de logs
â”‚   â”œâ”€â”€ app.log            # Logs de l'application
â”‚   â””â”€â”€ training.log       # Logs d'entraÃ®nement
â”‚
â”œâ”€â”€ frontend/              # Interface web
â”‚   â””â”€â”€ index.html         # Interface utilisateur
â”‚
â”œâ”€â”€ evaluate.py            # Script d'Ã©valuation du modÃ¨le
â”œâ”€â”€ inference.py           # Script d'infÃ©rence standalone
â”œâ”€â”€ requirements.txt       # DÃ©pendances Python
â”œâ”€â”€ .env                   # Variables d'environnement (inclus)
â””â”€â”€ README.md             # Ce fichier
```

## âš ï¸ Notes importantes

### ModÃ¨le rÃ©entraÃ®nÃ©

- **Le modÃ¨le issu du rÃ©entraÃ®nement Ã©crase le prÃ©cÃ©dent `best.pt`**
- Il n'y a pas de sauvegarde automatique des versions prÃ©cÃ©dentes
- Pour conserver une version, copiez `best.pt` avant un nouveau rÃ©entraÃ®nement

### Dataset et configuration

- Le dossier `dataset/` est **inclus** dans le projet pour faciliter la passation
- Le fichier `.env` est **inclus** avec les paramÃ¨tres par dÃ©faut
- **Aucune donnÃ©e sensible ou confidentielle** n'est prÃ©sente dans ces fichiers

### Production vs DÃ©veloppement

- **En production** : Les modÃ¨les finetunÃ©s (`best.pt`) et SAM seront dÃ©jÃ  prÃ©sents
- Les fallbacks (tÃ©lÃ©chargement automatique, entraÃ®nement initial) sont prÃ©vus pour :
  - Le dÃ©veloppement local
  - Les dÃ©ploiements initiaux
  - Les environnements de test

### Logs et dÃ©bogage

- Consultez `logs/app.log` pour les erreurs de l'application
- Consultez `logs/training.log` pour suivre l'entraÃ®nement
- Les logs incluent des informations dÃ©taillÃ©es sur les opÃ©rations

### Performance

- **GPU recommandÃ©** : L'entraÃ®nement et l'infÃ©rence sont beaucoup plus rapides avec un GPU NVIDIA
- **CPU** : Fonctionne mais peut Ãªtre trÃ¨s lent pour l'entraÃ®nement (plusieurs heures)
- **Apple Silicon** : Support MPS pour accÃ©lÃ©ration sur Mac avec puce Apple

## ğŸ”— Endpoints API principaux

- `POST /api/analyze` : Analyser une image
- `POST /api/segment` : Segmentation SAM guidÃ©e
- `POST /api/validate/from-segmentation` : Valider une image depuis SAM
- `POST /api/training/retrain` : Lancer un rÃ©entraÃ®nement
- `GET /api/training/status/<training_id>` : Statut d'un entraÃ®nement
- `POST /api/analyze/export-csv` : Exporter les rÃ©sultats en CSV

## ğŸ“ Licence

Voir le fichier de licence du projet.

## ğŸ‘¥ Support

Pour toute question ou problÃ¨me, consultez les logs dans `logs/` ou contactez l'Ã©quipe de dÃ©veloppement.

