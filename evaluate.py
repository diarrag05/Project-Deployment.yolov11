"""
Script d'√©valuation du mod√®le YOLOv11-segmentation
Calcule: mAP, pr√©cision, rappel, IoU sur le test set
"""

import os
from pathlib import Path
from ultralytics import YOLO
import torch
import json
from datetime import datetime

PROJECT_DIR = Path(__file__).parent
DATA_YAML = PROJECT_DIR / "data.yaml"
MODELS_DIR = PROJECT_DIR / "models"
EVAL_DIR = PROJECT_DIR / "evaluations"

# Cr√©er le r√©pertoire d'√©valuation
EVAL_DIR.mkdir(exist_ok=True)

def evaluate_model(model_path: str, task: str = "segment"):
    """
    √âvaluer le mod√®le YOLOv11
    
    Args:
        model_path: Chemin vers le mod√®le .pt
        task: Type de t√¢che (segment pour segmentation)
    
    Returns:
        R√©sultats d'√©valuation
    """
    
    print("=" * 80)
    print("üìä √âVALUATION DU MOD√àLE YOLOv11-SEGMENTATION")
    print("=" * 80)
    
    # V√©rifier le device
    device = 0 if torch.cuda.is_available() else "cpu"
    print(f"\nDevice: {device}")
    
    # Charger le mod√®le
    print(f"\nüì• Chargement du mod√®le: {model_path}")
    model = YOLO(model_path, task=task)
    
    # √âvaluer sur le test set
    print("\n‚è≥ √âvaluation sur le test set...")
    results = model.val(
        data=str(DATA_YAML),
        device=device,
        imgsz=640,
        batch=16,
        half=torch.cuda.is_available(),
        verbose=True,
    )
    
    # Afficher les r√©sultats
    print("\n" + "=" * 80)
    print("üìã R√âSULTATS D'√âVALUATION")
    print("=" * 80)
    
    metrics_dict = {
        "timestamp": datetime.now().isoformat(),
        "model_path": str(model_path),
        "device": str(device),
    }
    
    # M√©triques de segmentation
    if hasattr(results, 'box'):
        print("\nüéØ M√âTRIQUES DE D√âTECTION (Box):")
        print(f"  mAP50: {results.box.map50:.4f}")
        print(f"  mAP50-95: {results.box.map:.4f}")
        print(f"  Pr√©cision: {results.box.mp:.4f}")
        print(f"  Rappel: {results.box.mr:.4f}")
        
        metrics_dict['detection'] = {
            'mAP50': float(results.box.map50),
            'mAP50-95': float(results.box.map),
            'precision': float(results.box.mp),
            'recall': float(results.box.mr),
        }
    
    if hasattr(results, 'mask'):
        print("\nüé≠ M√âTRIQUES DE SEGMENTATION (Mask):")
        print(f"  mAP50: {results.mask.map50:.4f}")
        print(f"  mAP50-95: {results.mask.map:.4f}")
        print(f"  Pr√©cision: {results.mask.mp:.4f}")
        print(f"  Rappel: {results.mask.mr:.4f}")
        
        metrics_dict['segmentation'] = {
            'mAP50': float(results.mask.map50),
            'mAP50-95': float(results.mask.map),
            'precision': float(results.mask.mp),
            'recall': float(results.mask.mr),
        }
    
    # IoU par classe
    if hasattr(results, 'ious'):
        print("\nüìê IoU PAR CLASSE:")
        for i, iou in enumerate(results.ious):
            class_name = results.names[i] if hasattr(results, 'names') else f"Class {i}"
            print(f"  {class_name}: {iou:.4f}")
    
    # Confusion matrix
    if hasattr(results, 'confusion_matrix'):
        print("\nüî¢ CONFUSION MATRIX:")
        print(results.confusion_matrix)
    
    print("=" * 80)
    
    # Sauvegarder les r√©sultats en JSON
    results_file = EVAL_DIR / f"evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(results_file, 'w') as f:
        json.dump(metrics_dict, f, indent=4)
    print(f"\nüíæ R√©sultats sauvegard√©s: {results_file}")
    
    return results, metrics_dict

def evaluate_all_models():
    """√âvaluer tous les mod√®les disponibles"""
    if not MODELS_DIR.exists():
        print(f"‚ùå Dossier models n'existe pas: {MODELS_DIR}")
        return
    
    models = list(MODELS_DIR.glob("*.pt"))
    if not models:
        print(f"‚ö† Aucun mod√®le trouv√© dans: {MODELS_DIR}")
        return
    
    print(f"\nüìã {len(models)} mod√®le(s) trouv√©(s):")
    for i, model in enumerate(models, 1):
        print(f"  {i}. {model.name}")
    
    # √âvaluer chaque mod√®le
    results_summary = {}
    for model_path in models:
        print(f"\n{'=' * 80}")
        print(f"√âvaluation de: {model_path.name}")
        print(f"{'=' * 80}")
        
        try:
            results, metrics = evaluate_model(str(model_path))
            results_summary[model_path.name] = metrics
        except Exception as e:
            print(f"‚ùå Erreur lors de l'√©valuation: {e}")
    
    # Sauvegarder le r√©sum√©
    summary_file = EVAL_DIR / f"summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(summary_file, 'w') as f:
        json.dump(results_summary, f, indent=4)
    print(f"\nüíæ R√©sum√© sauvegard√©: {summary_file}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        # √âvaluer un mod√®le sp√©cifique
        model_path = sys.argv[1]
        evaluate_model(model_path)
    else:
        # √âvaluer tous les mod√®les
        evaluate_all_models()
