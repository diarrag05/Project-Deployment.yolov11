"""
Pipeline complet d'entra√Ænement, √©valuation et inf√©rence
Int√®gre automatiquement les meilleures pratiques
"""

import sys
from pathlib import Path
from datetime import datetime
import json

from train import train_yolov11_segmentation, print_metrics
from evaluate import evaluate_model
from inference import InferenceWithVoidRate
from config import (
    BALANCED_TRAINING,
    STANDARD_AUGMENTATION,
)

PROJECT_DIR = Path(__file__).parent
MODELS_DIR = PROJECT_DIR / "models"
LOGS_DIR = PROJECT_DIR / "logs"

LOGS_DIR.mkdir(exist_ok=True)

def log_pipeline_step(step_name: str, status: str, message: str = ""):
    """Enregistrer une √©tape du pipeline"""
    timestamp = datetime.now().isoformat()
    log_file = LOGS_DIR / "pipeline.log"
    
    log_entry = f"[{timestamp}] {step_name}: {status}"
    if message:
        log_entry += f" - {message}"
    
    print(log_entry)
    
    with open(log_file, "a") as f:
        f.write(log_entry + "\n")

def run_full_pipeline(
    use_config: str = "BALANCED",
    skip_training: bool = False,
    skip_evaluation: bool = False,
    skip_inference: bool = False,
    model_path: str = None,
):
    """
    Ex√©cuter le pipeline complet
    
    Args:
        use_config: Configuration √† utiliser (BALANCED, FAST, HIGH_QUALITY, etc.)
        skip_training: Sauter l'entra√Ænement
        skip_evaluation: Sauter l'√©valuation
        skip_inference: Sauter l'inf√©rence
        model_path: Chemin vers un mod√®le existant
    """
    
    print("=" * 80)
    print("üîÑ PIPELINE COMPLET YOLOv11-SEGMENTATION")
    print("=" * 80)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_log = {
        "timestamp": timestamp,
        "steps": {}
    }
    
    # ====== ENTRA√éNEMENT ======
    if not skip_training and not model_path:
        print("\n" + "=" * 80)
        print("1Ô∏è‚É£  ENTRA√éNEMENT")
        print("=" * 80)
        
        try:
            log_pipeline_step("TRAINING", "STARTED", f"Config: {use_config}")
            
            # Obtenir la configuration
            if use_config == "BALANCED":
                from config import BALANCED_TRAINING
                config = BALANCED_TRAINING.copy()
            else:
                from config import (
                    FAST_TRAINING,
                    HIGH_QUALITY_TRAINING,
                    PRODUCTION_TRAINING,
                )
                configs = {
                    "FAST": FAST_TRAINING,
                    "HIGH_QUALITY": HIGH_QUALITY_TRAINING,
                    "PRODUCTION": PRODUCTION_TRAINING,
                }
                config = configs.get(use_config, BALANCED_TRAINING).copy()
            
            # Ajouter augmentation
            config.update(STANDARD_AUGMENTATION)
            
            # Entra√Æner
            model, results, run_dir = train_yolov11_segmentation(**config)
            
            # R√©cup√©rer le chemin du meilleur mod√®le
            best_model = run_dir / "weights" / "best.pt"
            model_path = str(best_model) if best_model.exists() else None
            
            log_pipeline_step("TRAINING", "COMPLETED", f"Model: {model_path}")
            
            run_log["steps"]["training"] = {
                "status": "completed",
                "model": str(model_path),
                "run_dir": str(run_dir),
            }
            
        except Exception as e:
            log_pipeline_step("TRAINING", "FAILED", str(e))
            run_log["steps"]["training"] = {"status": "failed", "error": str(e)}
            print(f"\n‚ùå Erreur d'entra√Ænement: {e}")
            sys.exit(1)
    
    elif not model_path:
        # Chercher le meilleur mod√®le existant
        models = sorted(MODELS_DIR.glob("*.pt"), key=lambda x: x.stat().st_mtime, reverse=True)
        if models:
            model_path = str(models[0])
            print(f"\nüì¶ Utilisation du mod√®le existant: {Path(model_path).name}")
        else:
            print("‚ùå Aucun mod√®le trouv√©")
            sys.exit(1)
    
    # ====== √âVALUATION ======
    if not skip_evaluation and model_path:
        print("\n" + "=" * 80)
        print("2Ô∏è‚É£  √âVALUATION")
        print("=" * 80)
        
        try:
            log_pipeline_step("EVALUATION", "STARTED", f"Model: {Path(model_path).name}")
            
            results, metrics = evaluate_model(model_path)
            
            log_pipeline_step("EVALUATION", "COMPLETED")
            
            run_log["steps"]["evaluation"] = {
                "status": "completed",
                "metrics": metrics,
            }
            
        except Exception as e:
            log_pipeline_step("EVALUATION", "FAILED", str(e))
            run_log["steps"]["evaluation"] = {"status": "failed", "error": str(e)}
            print(f"\n‚ö† Erreur d'√©valuation (non bloquant): {e}")
    
    # ====== INF√âRENCE & VOID_RATE ======
    if not skip_inference and model_path:
        print("\n" + "=" * 80)
        print("3Ô∏è‚É£  INF√âRENCE & CALCUL DU TAUX DE VIDES")
        print("=" * 80)
        
        try:
            log_pipeline_step("INFERENCE", "STARTED", f"Model: {Path(model_path).name}")
            
            # Cr√©er l'inf√©rence
            inference = InferenceWithVoidRate(model_path, conf_threshold=0.5)
            
            # Traiter le test set
            test_images_dir = PROJECT_DIR / "test" / "images"
            if test_images_dir.exists():
                results = inference.infer_directory(str(test_images_dir))
                
                # Sauvegarder
                output_file = inference.save_results(results)
                
                # R√©sum√©
                inference.print_results_summary(results)
                
                log_pipeline_step("INFERENCE", "COMPLETED", f"Images: {len(results)}")
                
                run_log["steps"]["inference"] = {
                    "status": "completed",
                    "num_images": len(results),
                    "results_file": output_file,
                }
                
            else:
                print(f"‚ö† R√©pertoire test/images introuvable: {test_images_dir}")
        
        except Exception as e:
            log_pipeline_step("INFERENCE", "FAILED", str(e))
            run_log["steps"]["inference"] = {"status": "failed", "error": str(e)}
            print(f"\n‚ö† Erreur d'inf√©rence (non bloquant): {e}")
    
    # ====== R√âSUM√â ======
    print("\n" + "=" * 80)
    print("‚úÖ PIPELINE TERMIN√â")
    print("=" * 80)
    
    # Sauvegarder le log du pipeline
    pipeline_log_file = LOGS_DIR / f"pipeline_{timestamp}.json"
    with open(pipeline_log_file, "w") as f:
        json.dump(run_log, f, indent=4)
    
    print(f"\nüìù Logs: {pipeline_log_file}")
    print(f"üìä Mod√®le final: {model_path}")
    
    return model_path, run_log

def main():
    """Fonction principale"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Pipeline complet YOLOv11")
    parser.add_argument(
        "-c", "--config",
        default="BALANCED",
        choices=["FAST", "BALANCED", "HIGH_QUALITY", "PRODUCTION"],
        help="Configuration √† utiliser"
    )
    parser.add_argument(
        "-m", "--model",
        help="Chemin vers un mod√®le existant (saute l'entra√Ænement)"
    )
    parser.add_argument(
        "--skip-training",
        action="store_true",
        help="Sauter l'entra√Ænement"
    )
    parser.add_argument(
        "--skip-evaluation",
        action="store_true",
        help="Sauter l'√©valuation"
    )
    parser.add_argument(
        "--skip-inference",
        action="store_true",
        help="Sauter l'inf√©rence"
    )
    
    args = parser.parse_args()
    
    # Ex√©cuter le pipeline
    model_path, log = run_full_pipeline(
        use_config=args.config,
        skip_training=args.skip_training,
        skip_evaluation=args.skip_evaluation,
        skip_inference=args.skip_inference,
        model_path=args.model,
    )
    
    print("\nüéâ Pipeline ex√©cut√© avec succ√®s!")

if __name__ == "__main__":
    main()
